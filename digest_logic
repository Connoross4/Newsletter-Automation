import os
import sqlite3
import base64
import requests
from datetime import date
from bs4 import BeautifulSoup
from dotenv import load_dotenv

from langchain.agents import initialize_agent, Tool
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

# ---------------- Configuration ----------------
load_dotenv()
CLAUDE_KEY = os.getenv("CLAUDE_API_KEY")
if not CLAUDE_KEY:
    raise RuntimeError("CLAUDE_API_KEY not set in .env")

SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
DB_PATH = os.path.join(os.path.dirname(__file__), 'digests.db')
EMAIL_QUERY = 'newer_than:1d (from:no-reply@medium.com OR from:ayratmurtazin@mail.beehiiv.com)'

# ---------------- Database Setup ----------------
def init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY,
            source TEXT,
            title TEXT,
            link TEXT UNIQUE,
            summary TEXT,
            content TEXT,
            date TEXT
        )""")
    return conn

# ---------------- LLM & Chains ----------------
llm = ChatAnthropic(
    anthropic_api_key=CLAUDE_KEY,
    model_name="claude-3-7-sonnet-20250219",
    temperature=0.2
)
prompt = PromptTemplate(
    input_variables=["text"],
    template=(
        "You are an expert analyst. Summarize the following in 3 sentences:\n{text}"
    )
)
summarizer_chain = LLMChain(llm=llm, prompt=prompt)

# ---------------- Tool Definitions ----------------
def fetch_emails(_: str) -> list[tuple]:
    "Fetch raw HTML digests from Gmail. Returns list of (id, html)."
    creds = None
    token_file = 'token.json'
    creds_file = 'credentials.json'
    if os.path.exists(token_file):
        creds = Credentials.from_authorized_user_file(token_file, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(creds_file, SCOPES)
            creds = flow.run_local_server(port=0)
        with open(token_file, 'w') as f:
            f.write(creds.to_json())
    svc = build('gmail', 'v1', credentials=creds)
    resp = svc.users().messages().list(userId='me', q=EMAIL_QUERY).execute()
    results = []
    for msg in resp.get('messages', []):
        full = svc.users().messages().get(userId='me', id=msg['id'], format='full').execute()
        for part in full['payload'].get('parts', []):
            if part['mimeType']=='text/html' and part['body'].get('data'):
                html = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
                results.append((msg['id'], html))
                break
    return results

def extract_links(html: str) -> list[tuple]:
    "Parse HTML, return list of (title, url) for articles."
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for a in soup.find_all('a', href=True):
        text = a.get_text(strip=True)
        href = a['href']
        if 'medium.com' in href or 'beehiiv.com' in href:
            links.append((text, href))
    # dedupe
    seen = set(); unique=[]
    for t,u in links:
        if u not in seen:
            seen.add(u); unique.append((t,u))
    return unique

def fetch_article(_: str, url: str) -> str:
    "Download and clean article text."
    headers = {'User-Agent':'Mozilla/5.0'}
    r = requests.get(url, headers=headers, timeout=10)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, 'html.parser')
    return '\n'.join(p.get_text() for p in soup.find_all('p'))

def summarize_text(_: str, text: str) -> str:
    "Run summarizer chain."
    return summarizer_chain.invoke(text=text)

tools = [
    Tool(name="fetch_emails", func=fetch_emails, description="Get raw digest HTMLs."),
    Tool(name="extract_links", func=extract_links, description="Extract article links."),
    Tool(name="fetch_article", func=fetch_article, description="Download article content."),
    Tool(name="summarize_text", func=summarize_text, description="Summarize content.")
]

# ---------------- Agent Initialization ----------------
agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)

# ---------------- Main Workflow ----------------
def run_pipeline():
    db = init_db()
    cursor = db.cursor()
    # 1. Fetch emails
    emails = agent.run("fetch_emails()")
    for msg_id, html in emails:
        # 2. Extract links
        links = agent.run(f"extract_links({html!r})")
        for title,url in links:
            # 3. Get article text
            content = agent.run(f"fetch_article({url!r})")
            # 4. Summarize
            summary = agent.run(f"summarize_text({content!r})")
            # 5. Persist
            cursor.execute(
                "INSERT OR IGNORE INTO articles(source,title,link,summary,content,date) VALUES(?,?,?,?,?,?)",
                ("medium", title, url, summary, content, date.today().isoformat())
            )
    db.commit()
    print("âœ… Pipeline complete.")

if __name__ == "__main__":
    run_pipeline()
